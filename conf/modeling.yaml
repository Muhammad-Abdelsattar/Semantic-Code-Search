model:
  model_id: "jinaai/jina-embeddings-v2-base-code"
  fine_tuning:
    full:
      layers: 2 #indicates the num of layers(last layers) to be trained. {0->no layers -1->all layers}
      train_embeddings: false
    peft:
      lora:
        task_type: "SEQ_CLS"
        inference_mode: False
        target_modules: ["query", "value", "key"]
        r: 8
        lora_alpha: 32
        lora_dropout: 0.05
      prefix:
        task_type: "SEQ_CLS"
        num_virtual_tokens: 20
        inference_mode: False
        prefix_projection: True
loss:
  name: "InfoNCE"
  loss_args:
    memory_bank_size: 65536
    embedding_dim: 768
    temperature: 0.07
optimizer:
  optimizer:
    name: "AdamW"
    optimizer_args:
      weight_decay: 0.01
      betas: [0.9, 0.999]
  scheduler:
    name: "linear"
    scheduler_args:
      start_factor: 1/3
      end_factor: 1.0
      total_iters: 1000
  warmup:
    warmup_steps: 100 # or warmup_epochs
    start_factor: 1/3
