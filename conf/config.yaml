modeling:
  model:
    model_id: &id "jinaai/jina-embeddings-v2-base-code"
    fine_tuning:
      use: "full"

      full:
        layers: 2 #indicates the num of layers(last layers) to be trained. {0->no layers -1->all layers}
        train_embeddings: false
      peft:
        lora:
          task_type: "SEQ_CLS"
          inference_mode: False
          target_modules: ["query", "value", "key"]
          r: 8
          lora_alpha: 32
          lora_dropout: 0.05
        prefix:
          task_type: "SEQ_CLS"
          num_virtual_tokens: 20
          inference_mode: False
          prefix_projection: True
  loss:
    name: "InfoNCE"
    loss_args:
      temperature: 0.07
  optimizer:
    optimizer:
      name: "AdamW"
      optimizer_args:
        lr: 1e-4
        weight_decay: 0.01
        betas: [0.9, 0.999]
    scheduler:
      name: "linear"
      scheduler_args:
        start_factor: 0.3
        end_factor: 1.0
        total_iters: 1000
    warmup:
      warmup_steps: 50 # or warmup_epochs
      start_factor: 1/3

data:
  tokenizer_id: *id
  batch_size: 32
  train_val_files:
    - "../input/python/train.jsonl"
  train_split: 0.9

trainer:
  max_epochs: 10
  accumulate_grad_batches: 8
  precision: "32"
  gradient_clip_val: 0.5
  gradient_clip_algorithm: "norm"
  devices: "auto"
  accelerator: "auto"
  strategy: "auto"
