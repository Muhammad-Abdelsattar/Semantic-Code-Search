{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/semantic_code_search/code_search/modeling\n"
     ]
    }
   ],
   "source": [
    "%cd semantic_code_search/code_search/modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from enum import Enum, auto\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import (PeftModel, PeftConfig, PrefixTuningConfig, \n",
    "                 LoraConfig, get_peft_model, TaskType)\n",
    "from omegaconf import DictConfig\n",
    "from typing import Optional, Dict, Type\n",
    "from optimizers import *\n",
    "from model_manager import *\n",
    "from losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningType(Enum):\n",
    "    \"\"\"Supported Fine-Tuning methods.\"\"\"\n",
    "    PEFT = auto()\n",
    "    FULL = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'PEFT' is not a valid FineTuningType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m FineTuningType(\u001b[39m\"\u001b[39;49m\u001b[39mPEFT\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/enum.py:385\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39mEither returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39m`type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39mif\u001b[39;00m names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# simple value lookup\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__new__\u001b[39;49m(\u001b[39mcls\u001b[39;49m, value)\n\u001b[1;32m    386\u001b[0m \u001b[39m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_create_(\n\u001b[1;32m    388\u001b[0m         value,\n\u001b[1;32m    389\u001b[0m         names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         start\u001b[39m=\u001b[39mstart,\n\u001b[1;32m    394\u001b[0m         )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/enum.py:710\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    708\u001b[0m ve_exc \u001b[39m=\u001b[39m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m is not a valid \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (value, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m))\n\u001b[1;32m    709\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mraise\u001b[39;00m ve_exc\n\u001b[1;32m    711\u001b[0m \u001b[39melif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     exc \u001b[39m=\u001b[39m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    713\u001b[0m             \u001b[39m'\u001b[39m\u001b[39merror in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m._missing_: returned \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m instead of None or a valid member\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    714\u001b[0m             \u001b[39m%\u001b[39m (\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, result)\n\u001b[1;32m    715\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: 'PEFT' is not a valid FineTuningType"
     ]
    }
   ],
   "source": [
    "FineTuningType(\"PEFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = InfoNCELoss(temperature=0.07)\n",
    "bank = MemoryBank(size=1024,embedding_dim=128,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(8,128)\n",
    "k = torch.randn(8,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,logits = criterion(query_embeddings=q,key_embeddings=k,memory_bank=bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1032])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(\"/teamspace/studios/this_studio/semantic_code_search/conf/modeling.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config.model.model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = ModelManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_manager.prepare_model(model=model,fine_tuning_type=FineTuningType.FULL,config=config.model.fine_tuning,peft_type=PEFTType.LORA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (int, like=torch.dtype), but expected one of:\n * (Number end, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, *, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, Number step = 1, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49marange(\u001b[39m10\u001b[39;49m,like\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat)\n",
      "\u001b[0;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (int, like=torch.dtype), but expected one of:\n * (Number end, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, *, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, Number step = 1, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "torch.arange(10,like=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = len([name for name in prepared_model.named_modules() if \"encoder.layer\" in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('encoder.layer', ModuleList(\n",
      "  (0-11): 12 x JinaBertLayer(\n",
      "    (attention): JinaBertAttention(\n",
      "      (self): JinaBertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (layer_norm_q): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_norm_k): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (output): JinaBertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm_1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (layer_norm_2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (mlp): JinaBertGLUMLP(\n",
      "      (act): GELU(approximate='none')\n",
      "      (up_gated_layer): Linear(in_features=768, out_features=6144, bias=False)\n",
      "      (down_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "))]\n"
     ]
    }
   ],
   "source": [
    "print([name for name in prepared_model.named_modules() if \"encoder.layer\" in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mrequires_grad \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39melif\u001b[39;00m param\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m----> 7\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if any(layer in name for layer in trainable_layers):\n",
    "        assert param.requires_grad == True\n",
    "    elif \"embeddings\" in name:\n",
    "        assert param.requires_grad == False\n",
    "    elif param.requires_grad:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"embeddings\" not in name:\n",
    "        assert param.requires_grad == True\n",
    "    else:\n",
    "        assert param.requires_grad == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
