{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/semantic_code_search/code_search/modeling\n"
     ]
    }
   ],
   "source": [
    "%cd semantic_code_search/code_search/modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from enum import Enum, auto\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import (PeftModel, PeftConfig, PrefixTuningConfig, \n",
    "                 LoraConfig, get_peft_model, TaskType)\n",
    "from omegaconf import DictConfig\n",
    "from typing import Optional, Dict, Type\n",
    "from optimizers import *\n",
    "from model_manager import *\n",
    "from losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = InfoNCELoss(temperature=0.07)\n",
    "bank = MemoryBank(size=1024,embedding_dim=128,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(8,128)\n",
    "k = torch.randn(8,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,logits = criterion(query_embeddings=q,key_embeddings=k,memory_bank=bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1032])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(\"/teamspace/studios/this_studio/semantic_code_search/conf/modeling.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config.model.model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = ModelManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_manager.prepare_model(model=model,fine_tuning_type=FineTuningType.FULL,config=config.model.fine_tuning,peft_type=PEFTType.LORA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (int, like=torch.dtype), but expected one of:\n * (Number end, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, *, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, Number step = 1, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49marange(\u001b[39m10\u001b[39;49m,like\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat)\n",
      "\u001b[0;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (int, like=torch.dtype), but expected one of:\n * (Number end, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, *, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, Number step = 1, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "torch.arange(10,like=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = len([name for name in prepared_model.named_modules() if \"encoder.layer\" in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('encoder.layer', ModuleList(\n",
      "  (0-11): 12 x JinaBertLayer(\n",
      "    (attention): JinaBertAttention(\n",
      "      (self): JinaBertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (layer_norm_q): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_norm_k): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (output): JinaBertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm_1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (layer_norm_2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (mlp): JinaBertGLUMLP(\n",
      "      (act): GELU(approximate='none')\n",
      "      (up_gated_layer): Linear(in_features=768, out_features=6144, bias=False)\n",
      "      (down_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "))]\n"
     ]
    }
   ],
   "source": [
    "print([name for name in prepared_model.named_modules() if \"encoder.layer\" in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mrequires_grad \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39melif\u001b[39;00m param\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m----> 7\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if any(layer in name for layer in trainable_layers):\n",
    "        assert param.requires_grad == True\n",
    "    elif \"embeddings\" in name:\n",
    "        assert param.requires_grad == False\n",
    "    elif param.requires_grad:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"embeddings\" not in name:\n",
    "        assert param.requires_grad == True\n",
    "    else:\n",
    "        assert param.requires_grad == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
