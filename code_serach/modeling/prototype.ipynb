{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/semantic_code_search/code_serach/modeling\n"
     ]
    }
   ],
   "source": [
    "%cd semantic_code_search/code_serach/modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from enum import Enum, auto\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import (PeftModel, PeftConfig, PrefixTuningConfig, \n",
    "                 LoraConfig, get_peft_model, TaskType)\n",
    "from omegaconf import DictConfig\n",
    "from typing import Optional, Dict, Type\n",
    "from model_manager import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(\"/teamspace/studios/this_studio/semantic_code_search/conf/modeling.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config.model.model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = ModelManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_manager.prepare_model(model=model,fine_tuning_type=FineTuningType.FULL,config=config.model.fine_tuning,peft_type=PEFTType.LORA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight False\n",
      "embeddings.token_type_embeddings.weight False\n",
      "embeddings.LayerNorm.weight False\n",
      "embeddings.LayerNorm.bias False\n",
      "encoder.layer.0.attention.self.query.weight False\n",
      "encoder.layer.0.attention.self.query.bias False\n",
      "encoder.layer.0.attention.self.key.weight False\n",
      "encoder.layer.0.attention.self.key.bias False\n",
      "encoder.layer.0.attention.self.value.weight False\n",
      "encoder.layer.0.attention.self.value.bias False\n",
      "encoder.layer.0.attention.self.layer_norm_q.weight False\n",
      "encoder.layer.0.attention.self.layer_norm_q.bias False\n",
      "encoder.layer.0.attention.self.layer_norm_k.weight False\n",
      "encoder.layer.0.attention.self.layer_norm_k.bias False\n",
      "encoder.layer.0.attention.output.dense.weight False\n",
      "encoder.layer.0.attention.output.dense.bias False\n",
      "encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "encoder.layer.0.layer_norm_1.weight False\n",
      "encoder.layer.0.layer_norm_1.bias False\n",
      "encoder.layer.0.layer_norm_2.weight False\n",
      "encoder.layer.0.layer_norm_2.bias False\n",
      "encoder.layer.0.mlp.up_gated_layer.weight False\n",
      "encoder.layer.0.mlp.down_layer.weight False\n",
      "encoder.layer.0.mlp.down_layer.bias False\n",
      "encoder.layer.1.attention.self.query.weight False\n",
      "encoder.layer.1.attention.self.query.bias False\n",
      "encoder.layer.1.attention.self.key.weight False\n",
      "encoder.layer.1.attention.self.key.bias False\n",
      "encoder.layer.1.attention.self.value.weight False\n",
      "encoder.layer.1.attention.self.value.bias False\n",
      "encoder.layer.1.attention.self.layer_norm_q.weight False\n",
      "encoder.layer.1.attention.self.layer_norm_q.bias False\n",
      "encoder.layer.1.attention.self.layer_norm_k.weight False\n",
      "encoder.layer.1.attention.self.layer_norm_k.bias False\n",
      "encoder.layer.1.attention.output.dense.weight False\n",
      "encoder.layer.1.attention.output.dense.bias False\n",
      "encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "encoder.layer.1.layer_norm_1.weight False\n",
      "encoder.layer.1.layer_norm_1.bias False\n",
      "encoder.layer.1.layer_norm_2.weight False\n",
      "encoder.layer.1.layer_norm_2.bias False\n",
      "encoder.layer.1.mlp.up_gated_layer.weight False\n",
      "encoder.layer.1.mlp.down_layer.weight False\n",
      "encoder.layer.1.mlp.down_layer.bias False\n",
      "encoder.layer.2.attention.self.query.weight False\n",
      "encoder.layer.2.attention.self.query.bias False\n",
      "encoder.layer.2.attention.self.key.weight False\n",
      "encoder.layer.2.attention.self.key.bias False\n",
      "encoder.layer.2.attention.self.value.weight False\n",
      "encoder.layer.2.attention.self.value.bias False\n",
      "encoder.layer.2.attention.self.layer_norm_q.weight False\n",
      "encoder.layer.2.attention.self.layer_norm_q.bias False\n",
      "encoder.layer.2.attention.self.layer_norm_k.weight False\n",
      "encoder.layer.2.attention.self.layer_norm_k.bias False\n",
      "encoder.layer.2.attention.output.dense.weight False\n",
      "encoder.layer.2.attention.output.dense.bias False\n",
      "encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "encoder.layer.2.layer_norm_1.weight False\n",
      "encoder.layer.2.layer_norm_1.bias False\n",
      "encoder.layer.2.layer_norm_2.weight False\n",
      "encoder.layer.2.layer_norm_2.bias False\n",
      "encoder.layer.2.mlp.up_gated_layer.weight False\n",
      "encoder.layer.2.mlp.down_layer.weight False\n",
      "encoder.layer.2.mlp.down_layer.bias False\n",
      "encoder.layer.3.attention.self.query.weight False\n",
      "encoder.layer.3.attention.self.query.bias False\n",
      "encoder.layer.3.attention.self.key.weight False\n",
      "encoder.layer.3.attention.self.key.bias False\n",
      "encoder.layer.3.attention.self.value.weight False\n",
      "encoder.layer.3.attention.self.value.bias False\n",
      "encoder.layer.3.attention.self.layer_norm_q.weight False\n",
      "encoder.layer.3.attention.self.layer_norm_q.bias False\n",
      "encoder.layer.3.attention.self.layer_norm_k.weight False\n",
      "encoder.layer.3.attention.self.layer_norm_k.bias False\n",
      "encoder.layer.3.attention.output.dense.weight False\n",
      "encoder.layer.3.attention.output.dense.bias False\n",
      "encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "encoder.layer.3.layer_norm_1.weight False\n",
      "encoder.layer.3.layer_norm_1.bias False\n",
      "encoder.layer.3.layer_norm_2.weight False\n",
      "encoder.layer.3.layer_norm_2.bias False\n",
      "encoder.layer.3.mlp.up_gated_layer.weight False\n",
      "encoder.layer.3.mlp.down_layer.weight False\n",
      "encoder.layer.3.mlp.down_layer.bias False\n",
      "encoder.layer.4.attention.self.query.weight False\n",
      "encoder.layer.4.attention.self.query.bias False\n",
      "encoder.layer.4.attention.self.key.weight False\n",
      "encoder.layer.4.attention.self.key.bias False\n",
      "encoder.layer.4.attention.self.value.weight False\n",
      "encoder.layer.4.attention.self.value.bias False\n",
      "encoder.layer.4.attention.self.layer_norm_q.weight False\n",
      "encoder.layer.4.attention.self.layer_norm_q.bias False\n",
      "encoder.layer.4.attention.self.layer_norm_k.weight False\n",
      "encoder.layer.4.attention.self.layer_norm_k.bias False\n",
      "encoder.layer.4.attention.output.dense.weight False\n",
      "encoder.layer.4.attention.output.dense.bias False\n",
      "encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "encoder.layer.4.layer_norm_1.weight False\n",
      "encoder.layer.4.layer_norm_1.bias False\n",
      "encoder.layer.4.layer_norm_2.weight False\n",
      "encoder.layer.4.layer_norm_2.bias False\n",
      "encoder.layer.4.mlp.up_gated_layer.weight False\n",
      "encoder.layer.4.mlp.down_layer.weight False\n",
      "encoder.layer.4.mlp.down_layer.bias False\n",
      "encoder.layer.5.attention.self.query.weight False\n",
      "encoder.layer.5.attention.self.query.bias False\n",
      "encoder.layer.5.attention.self.key.weight False\n",
      "encoder.layer.5.attention.self.key.bias False\n",
      "encoder.layer.5.attention.self.value.weight False\n",
      "encoder.layer.5.attention.self.value.bias False\n",
      "encoder.layer.5.attention.self.layer_norm_q.weight False\n",
      "encoder.layer.5.attention.self.layer_norm_q.bias False\n",
      "encoder.layer.5.attention.self.layer_norm_k.weight False\n",
      "encoder.layer.5.attention.self.layer_norm_k.bias False\n",
      "encoder.layer.5.attention.output.dense.weight False\n",
      "encoder.layer.5.attention.output.dense.bias False\n",
      "encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "encoder.layer.5.layer_norm_1.weight False\n",
      "encoder.layer.5.layer_norm_1.bias False\n",
      "encoder.layer.5.layer_norm_2.weight False\n",
      "encoder.layer.5.layer_norm_2.bias False\n",
      "encoder.layer.5.mlp.up_gated_layer.weight False\n",
      "encoder.layer.5.mlp.down_layer.weight False\n",
      "encoder.layer.5.mlp.down_layer.bias False\n",
      "encoder.layer.6.attention.self.query.weight False\n",
      "encoder.layer.6.attention.self.query.bias False\n",
      "encoder.layer.6.attention.self.key.weight False\n",
      "encoder.layer.6.attention.self.key.bias False\n",
      "encoder.layer.6.attention.self.value.weight False\n",
      "encoder.layer.6.attention.self.value.bias False\n",
      "encoder.layer.6.attention.self.layer_norm_q.weight False\n",
      "encoder.layer.6.attention.self.layer_norm_q.bias False\n",
      "encoder.layer.6.attention.self.layer_norm_k.weight False\n",
      "encoder.layer.6.attention.self.layer_norm_k.bias False\n",
      "encoder.layer.6.attention.output.dense.weight False\n",
      "encoder.layer.6.attention.output.dense.bias False\n",
      "encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "encoder.layer.6.layer_norm_1.weight False\n",
      "encoder.layer.6.layer_norm_1.bias False\n",
      "encoder.layer.6.layer_norm_2.weight False\n",
      "encoder.layer.6.layer_norm_2.bias False\n",
      "encoder.layer.6.mlp.up_gated_layer.weight False\n",
      "encoder.layer.6.mlp.down_layer.weight False\n",
      "encoder.layer.6.mlp.down_layer.bias False\n",
      "encoder.layer.7.attention.self.query.weight False\n",
      "encoder.layer.7.attention.self.query.bias False\n",
      "encoder.layer.7.attention.self.key.weight False\n",
      "encoder.layer.7.attention.self.key.bias False\n",
      "encoder.layer.7.attention.self.value.weight False\n",
      "encoder.layer.7.attention.self.value.bias False\n",
      "encoder.layer.7.attention.self.layer_norm_q.weight False\n",
      "encoder.layer.7.attention.self.layer_norm_q.bias False\n",
      "encoder.layer.7.attention.self.layer_norm_k.weight False\n",
      "encoder.layer.7.attention.self.layer_norm_k.bias False\n",
      "encoder.layer.7.attention.output.dense.weight False\n",
      "encoder.layer.7.attention.output.dense.bias False\n",
      "encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "encoder.layer.7.layer_norm_1.weight False\n",
      "encoder.layer.7.layer_norm_1.bias False\n",
      "encoder.layer.7.layer_norm_2.weight False\n",
      "encoder.layer.7.layer_norm_2.bias False\n",
      "encoder.layer.7.mlp.up_gated_layer.weight False\n",
      "encoder.layer.7.mlp.down_layer.weight False\n",
      "encoder.layer.7.mlp.down_layer.bias False\n",
      "encoder.layer.8.attention.self.query.weight False\n",
      "encoder.layer.8.attention.self.query.bias False\n",
      "encoder.layer.8.attention.self.key.weight False\n",
      "encoder.layer.8.attention.self.key.bias False\n",
      "encoder.layer.8.attention.self.value.weight False\n",
      "encoder.layer.8.attention.self.value.bias False\n",
      "encoder.layer.8.attention.self.layer_norm_q.weight False\n",
      "encoder.layer.8.attention.self.layer_norm_q.bias False\n",
      "encoder.layer.8.attention.self.layer_norm_k.weight False\n",
      "encoder.layer.8.attention.self.layer_norm_k.bias False\n",
      "encoder.layer.8.attention.output.dense.weight False\n",
      "encoder.layer.8.attention.output.dense.bias False\n",
      "encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "encoder.layer.8.layer_norm_1.weight False\n",
      "encoder.layer.8.layer_norm_1.bias False\n",
      "encoder.layer.8.layer_norm_2.weight False\n",
      "encoder.layer.8.layer_norm_2.bias False\n",
      "encoder.layer.8.mlp.up_gated_layer.weight False\n",
      "encoder.layer.8.mlp.down_layer.weight False\n",
      "encoder.layer.8.mlp.down_layer.bias False\n",
      "encoder.layer.9.attention.self.query.weight False\n",
      "encoder.layer.9.attention.self.query.bias False\n",
      "encoder.layer.9.attention.self.key.weight False\n",
      "encoder.layer.9.attention.self.key.bias False\n",
      "encoder.layer.9.attention.self.value.weight False\n",
      "encoder.layer.9.attention.self.value.bias False\n",
      "encoder.layer.9.attention.self.layer_norm_q.weight False\n",
      "encoder.layer.9.attention.self.layer_norm_q.bias False\n",
      "encoder.layer.9.attention.self.layer_norm_k.weight False\n",
      "encoder.layer.9.attention.self.layer_norm_k.bias False\n",
      "encoder.layer.9.attention.output.dense.weight False\n",
      "encoder.layer.9.attention.output.dense.bias False\n",
      "encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "encoder.layer.9.layer_norm_1.weight False\n",
      "encoder.layer.9.layer_norm_1.bias False\n",
      "encoder.layer.9.layer_norm_2.weight False\n",
      "encoder.layer.9.layer_norm_2.bias False\n",
      "encoder.layer.9.mlp.up_gated_layer.weight False\n",
      "encoder.layer.9.mlp.down_layer.weight False\n",
      "encoder.layer.9.mlp.down_layer.bias False\n",
      "encoder.layer.10.attention.self.query.weight True\n",
      "encoder.layer.10.attention.self.query.bias True\n",
      "encoder.layer.10.attention.self.key.weight True\n",
      "encoder.layer.10.attention.self.key.bias True\n",
      "encoder.layer.10.attention.self.value.weight True\n",
      "encoder.layer.10.attention.self.value.bias True\n",
      "encoder.layer.10.attention.self.layer_norm_q.weight True\n",
      "encoder.layer.10.attention.self.layer_norm_q.bias True\n",
      "encoder.layer.10.attention.self.layer_norm_k.weight True\n",
      "encoder.layer.10.attention.self.layer_norm_k.bias True\n",
      "encoder.layer.10.attention.output.dense.weight True\n",
      "encoder.layer.10.attention.output.dense.bias True\n",
      "encoder.layer.10.attention.output.LayerNorm.weight True\n",
      "encoder.layer.10.attention.output.LayerNorm.bias True\n",
      "encoder.layer.10.layer_norm_1.weight True\n",
      "encoder.layer.10.layer_norm_1.bias True\n",
      "encoder.layer.10.layer_norm_2.weight True\n",
      "encoder.layer.10.layer_norm_2.bias True\n",
      "encoder.layer.10.mlp.up_gated_layer.weight True\n",
      "encoder.layer.10.mlp.down_layer.weight True\n",
      "encoder.layer.10.mlp.down_layer.bias True\n",
      "encoder.layer.11.attention.self.query.weight True\n",
      "encoder.layer.11.attention.self.query.bias True\n",
      "encoder.layer.11.attention.self.key.weight True\n",
      "encoder.layer.11.attention.self.key.bias True\n",
      "encoder.layer.11.attention.self.value.weight True\n",
      "encoder.layer.11.attention.self.value.bias True\n",
      "encoder.layer.11.attention.self.layer_norm_q.weight True\n",
      "encoder.layer.11.attention.self.layer_norm_q.bias True\n",
      "encoder.layer.11.attention.self.layer_norm_k.weight True\n",
      "encoder.layer.11.attention.self.layer_norm_k.bias True\n",
      "encoder.layer.11.attention.output.dense.weight True\n",
      "encoder.layer.11.attention.output.dense.bias True\n",
      "encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "encoder.layer.11.layer_norm_1.weight True\n",
      "encoder.layer.11.layer_norm_1.bias True\n",
      "encoder.layer.11.layer_norm_2.weight True\n",
      "encoder.layer.11.layer_norm_2.bias True\n",
      "encoder.layer.11.mlp.up_gated_layer.weight True\n",
      "encoder.layer.11.mlp.down_layer.weight True\n",
      "encoder.layer.11.mlp.down_layer.bias True\n",
      "pooler.dense.weight True\n",
      "pooler.dense.bias True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prepared_model = model_manager.prepare_model(model=model,\n",
    "                                                config=config.model.fine_tuning,\n",
    "                                                fine_tuning_type=FineTuningType.FULL,\n",
    "                                                peft_type=PEFTType.PREFIX)\n",
    "assert isinstance(prepared_model, torch.nn.Module)\n",
    "assert hasattr(prepared_model, 'base_model')\n",
    "for name, param in prepared_model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "    # if \"prompt_encoder\" not in name:\n",
    "    #     assert param.requires_grad == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JinaBertModel(\n",
       "  (embeddings): JinaBertEmbeddings(\n",
       "    (word_embeddings): Embedding(61056, 768, padding_idx=0)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): JinaBertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x JinaBertLayer(\n",
       "        (attention): JinaBertAttention(\n",
       "          (self): JinaBertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (layer_norm_q): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_norm_k): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): JinaBertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (mlp): JinaBertGLUMLP(\n",
       "          (act): GELU(approximate='none')\n",
       "          (up_gated_layer): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (down_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): JinaBertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = len([name for name in prepared_model.named_modules() if \"encoder.layer\" in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('encoder.layer', ModuleList(\n",
      "  (0-11): 12 x JinaBertLayer(\n",
      "    (attention): JinaBertAttention(\n",
      "      (self): JinaBertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (layer_norm_q): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_norm_k): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (output): JinaBertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm_1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (layer_norm_2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (mlp): JinaBertGLUMLP(\n",
      "      (act): GELU(approximate='none')\n",
      "      (up_gated_layer): Linear(in_features=768, out_features=6144, bias=False)\n",
      "      (down_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "))]\n"
     ]
    }
   ],
   "source": [
    "print([name for name in prepared_model.named_modules() if \"encoder.layer\" in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mrequires_grad \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39melif\u001b[39;00m param\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m----> 7\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if any(layer in name for layer in trainable_layers):\n",
    "        assert param.requires_grad == True\n",
    "    elif \"embeddings\" in name:\n",
    "        assert param.requires_grad == False\n",
    "    elif param.requires_grad:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"embeddings\" not in name:\n",
    "        assert param.requires_grad == True\n",
    "    else:\n",
    "        assert param.requires_grad == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
